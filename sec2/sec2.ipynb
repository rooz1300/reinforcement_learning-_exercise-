{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)\n",
    "\n",
    "In this response, we will expand on the concepts and provide more details about MDPs.\n",
    "\n",
    "## What is a Markov Decision Process?\n",
    "\n",
    "An MDP is a mathematical framework for modeling decision-making in situations where the outcome of an action is uncertain. It is a type of stochastic process, which is a sequence of random variables that evolve over time. The key feature of an MDP is that the current state of the system completely determines the probabilities of transitioning to other states. This is known as the Markov property.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The mathematical formulation of an MDP involves defining a tuple $(S, A, P, R, \\gamma)$, where:\n",
    "\n",
    "1. $S$ is a set of states. A state $s \\in S$ represents a possible configuration of the system.\n",
    "2. $A$ is a set of actions. An action $a \\in A$ represents a possible decision that the agent can make.\n",
    "3. $P$ is the state transition probability matrix. $P(s'|s, a)$ is the probability of transitioning to state $s'$ given that the current state is $s$ and the agent takes action $a$.\n",
    "4. $R$ is the reward function. $R(s, a, s')$ is the reward that the agent receives for transitioning from state $s$ to state $s'$ when it takes action $a$.\n",
    "5. $\\gamma$ is the discount factor. It is a value between 0 and 1 that determines the importance of future rewards.\n",
    "\n",
    "## Goal of an MDP\n",
    "\n",
    "The goal of an agent in an MDP is to find a policy $\\pi$ that maximizes the expected sum of discounted rewards, starting from any state $s \\in S$. A policy $\\pi$ is a function that maps states to actions, $\\pi: S \\rightarrow A$.\n",
    "\n",
    "## Value Function and Optimal Policy\n",
    "\n",
    "The value function of a policy $\\pi$, denoted $v_\\pi(s)$, is the expected sum of discounted rewards that the agent will receive if it starts in state $s$ and follows policy $\\pi$ thereafter. It is defined as:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t=s\\right]$$\n",
    "\n",
    "The optimal value function, denoted $v_*(s)$, is the maximum possible value function over all policies. It is the solution to the Bellman optimality equation:\n",
    "\n",
    "$$v_*(s) = \\max_a \\sum_{s'} P(s'|s,a) (R(s,a,s') + \\gamma v_*(s'))$$\n",
    "\n",
    "The optimal policy $\\pi_*$ can be derived from the optimal value function as:\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) (R(s,a,s') + \\gamma v_*(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Markov Decision Processes (MDPs) - Value Function Equation\n",
    "$$v_{\\pi}(s)~\\doteq~\\mathbb{E}_{\\pi}[G_{t}\\mid S_{t}=s]~=~\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\mid S_{t}=s\\right],{\\mathrm{~for~all~}}s\\in{S},$$\n",
    "\n",
    "### Breakdown of the Equation\n",
    "\n",
    "$${v}_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_{t}\\mid S_{t}=s]$$\n",
    "\n",
    "#### Components of the Equation\n",
    "\n",
    "1. **Value Function**: $${v}_{\\pi}(s)$$ - The value function of a policy $\\pi$. It measures how good it is for an agent to be in a particular state $s$ under the policy $\\pi$.\n",
    "2. **Expected Return**: $$\\mathbb{E}_{\\pi}[G_{t}\\mid S_{t}=s]$$ - The expected value of the return $G_{t}$ given that the agent is in state $s$ at time $t$ and it follows the policy $\\pi$. The return $G_{t}$ is the sum of all future rewards that the agent will receive from time $t$.\n",
    "3. **Return Calculation**: $$\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+1+k}\\mid S_{t}=s\\right]$$ - The mathematical representation of the expected return. The agent is in state $s$ at time $t$. It follows the policy $\\pi$ and receives a reward $R_{t+k+1}$ at time $t+k+1$. The sum is over all future time steps ($k$ goes from 0 to infinity). The discount factor $\\gamma^{k}$ gives less importance to rewards that are far in the future.\n",
    "\n",
    "### Return Calculation\n",
    "\n",
    "In reinforcement learning, the return $G$ often denotes the total discounted reward that an agent receives from a certain time step until the end of the episode or the infinite future.\n",
    "\n",
    "For the equation provided, $G_{t}$ is the return that the agent receives from time step $t$ onwards. It is defined as the sum of all future discounted rewards:\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... = \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}$$\n",
    "\n",
    "Here, $R_{t+k+1}$ is the reward that the agent receives at time step $t+k+1$, and $\\gamma$ is the discount factor that determines the importance of future rewards. The discount factor is a value between 0 and 1, and the smaller it is, the less the agent cares about rewards in the distant future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Value Function Formulation**\n",
    "===============================\n",
    "\n",
    "### Definition\n",
    "\n",
    "The optimal value function $v_*(s)$ is defined as the maximum expected return that an agent can achieve when starting from state $s \\in S$, where $S$ is the set of states. This is denoted by:\n",
    "\n",
    "$$v_*(s) = \\max_\\pi v_\\pi(s),\\quad \\forall s \\in S$$\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "Let $\\mathcal{P}(s, a)$ be the transition probability function that maps a state-action pair $(s, a)$ to a probability distribution over next states $S$. The expected return function $v_\\pi(s)$ for policy $\\pi$ is defined as:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t | s_0 = s]$$\n",
    "\n",
    "where $r_t$ is the reward at time step $t$, $\\gamma$ is the discount factor, and $\\mathcal{P}(s, a)$ is used to compute the expected value of the return.\n",
    "\n",
    "The optimal value function $v_*(s)$ can be rewritten as:\n",
    "\n",
    "$$v_*(s) = \\max_\\pi \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t | s_0 = s]$$\n",
    "\n",
    "This formulation is often referred to as the Bellman Expectation Equation.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The optimal value function $v_*(s)$ represents the maximum expected return that an agent can achieve when starting from state $s$. This means that if an agent follows a policy $\\pi^*$ that achieves this maximum return, it will be able to maximize its expected reward in all states $s \\in S$.\n",
    "\n",
    "### Applications\n",
    "\n",
    "The optimal value function formulation has numerous applications in reinforcement learning and decision-making under uncertainty. It serves as the foundation for many algorithms, including dynamic programming, value iteration, and policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Action Value Function\n",
    "\n",
    "The state-action value function, often denoted as $Q(s,a)$, is a fundamental concept in reinforcement learning. It represents the expected return when an agent takes a specific action $a$ in a specific state $s$, and then follows a policy $\\pi$ thereafter.\n",
    "\n",
    "## Mathematical Representation\n",
    "\n",
    "The state-action value function can be mathematically represented as:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi[\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} | s_t = s, a_t = a]$$\n",
    "\n",
    "where:\n",
    "- $Q^\\pi(s,a)$ is the state-action value function for policy $\\pi$\n",
    "- $s_t$ is the state at time $t$\n",
    "- $a_t$ is the action taken at time $t$\n",
    "- $r_{t+k+1}$ is the reward received $k+1$ time-steps after time $t$\n",
    "- $\\gamma$ is the discount factor that determines the importance of future rewards\n",
    "- $\\mathbb{E}_\\pi$ denotes the expected value given that the agent follows policy $\\pi$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "In essence, the state-action value function provides a measure of the \"goodness\" of taking a particular action in a particular state, considering not just the immediate reward, but also the long-term rewards that might be obtained by following the policy $\\pi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation\n",
    "\n",
    "The **Optimal Value Function** $v_*(s)$ is defined as the solution to the following equation:\n",
    "\n",
    "$$v_*(s) = \\max_a \\sum_{s'} P(s'|s, a) [R(s, a, s') + \\gamma v_*(s')]$$\n",
    "\n",
    "where:\n",
    "* $v_*(s)$ is the maximum expected return starting from state $s$ and following an optimal policy.\n",
    "* $\\max_a$ takes the maximum over all possible actions $a$ in state $s$.\n",
    "* $\\sum_{s'} P(s'|s, a)$ is the probability of transitioning to state $s'$ given that the current state is $s$ and the agent takes action $a$.\n",
    "* $R(s, a, s')$ is the reward received for transitioning from state $s$ to state $s'$ when taking action $a$.\n",
    "* $\\gamma$ is the discount factor, which determines the importance of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! The **Frozen Lake** environment in OpenAI Gym is a classic toy text environment designed for reinforcement learning tasks. Let me break down the key details for you:\n",
    "\n",
    "1. **Objective**:\n",
    "   - The agent's goal is to navigate from the **Start (S)** position to the **Goal (G)** position on a grid.\n",
    "   - The grid represents a frozen lake, and the agent must avoid falling into holes while crossing.\n",
    "\n",
    "2. **Grid Description**:\n",
    "   - The grid is represented as a 4x4 matrix.\n",
    "   - Each cell can be one of the following:\n",
    "     - **F**: Frozen (safe to walk on)\n",
    "     - **H**: Hole (falling into a hole results in failure)\n",
    "     - **S**: Start position\n",
    "     - **G**: Goal position\n",
    "\n",
    "3. **Actions**:\n",
    "   - The agent can take four discrete actions:\n",
    "     - **0**: Move left\n",
    "     - **1**: Move down\n",
    "     - **2**: Move right\n",
    "     - **3**: Move up\n",
    "\n",
    "4. **Observation Space**:\n",
    "   - The observation is a value representing the agent's current position.\n",
    "   - For a 4x4 grid, the number of possible observations is 16 (from 0 to 15).\n",
    "\n",
    "5. **Rewards**:\n",
    "   - The reward schedule is as follows:\n",
    "     - Reach the goal (G): +1 reward\n",
    "     - Fall into a hole (H): 0 reward\n",
    "     - Step on frozen ice (F): 0 reward\n",
    "\n",
    "6. **Slippery Nature**:\n",
    "   - Due to the slippery nature of the frozen lake, the agent may not always move in the intended direction.\n",
    "   - If the environment is slippery, the agent has a 1/3 probability of moving in the intended direction and a 1/3 probability of moving in either perpendicular direction.\n",
    "\n",
    "7. **Environment Creation**:\n",
    "   - To create the Frozen Lake environment in OpenAI Gym, use:\n",
    "     ```python\n",
    "     import gym\n",
    "     env = gym.make(\"FrozenLake-v1\")\n",
    "     ```\n",
    "\n",
    "8. **Custom Maps**:\n",
    "   - You can specify custom maps for the frozen lake using the `desc` argument.\n",
    "   - For example:\n",
    "     ```python\n",
    "     custom_map = [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "     env = gym.make(\"FrozenLake-v1\", desc=custom_map)\n",
    "     ```\n",
    "\n",
    "9. **Random Maps**:\n",
    "   - To generate a random map, use the `generate_random_map` function:\n",
    "     ```python\n",
    "     from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "     env = gym.make(\"FrozenLake-v1\", desc=generate_random_map(size=8))\n",
    "     ```\n",
    "\n",
    "10. **Version History**:\n",
    "    - v1: Bug fixes to rewards\n",
    "    - v0: Initial release⁴.\n",
    "\n",
    "Feel free to explore this environment and experiment with reinforcement learning algorithms! 😊\n",
    "\n",
    "Source: Conversation with Copilot, 5/26/2024\n",
    "(1) Frozen Lake - Gym Documentation. https://www.gymlibrary.dev/environments/toy_text/frozen_lake/.\n",
    "(2) Introduction to OpenAI Gym and Frozen Lake Environment in Python- Reinforcement Learning Tutorial. https://www.youtube.com/watch?v=Vrro7W7iW2w.\n",
    "(3) OpenAI Gym: How to Start an Environment and Visualize it. https://www.youtube.com/watch?v=XFqGBnXzAoE.\n",
    "(4) Frozen Lake: Beginners Guide To Reinforcement Learning With OpenAI Gym. https://analyticsindiamag.com/openai-gym-frozen-lake-beginners-guide-reinforcement-learning/.\n",
    "(5) How to generate a random frozen lake map in OpenAI?. https://stackoverflow.com/questions/55006689/how-to-generate-a-random-frozen-lake-map-in-openai.\n",
    "(6) Train AI to solve the ️Frozen Lake environment using OpenAI Gym .... https://github.com/mayhazali/OpenAIGym-FrozenLake.\n",
    "(7) Gym Tutorial: The Frozen Lake - DEV Community. https://dev.to/rodolfomendes/gym-tutorial-the-frozen-lake-45al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Reset the environment to the initial state\n",
    "env.reset()\n",
    "\n",
    "# Render the environment to the screen\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the Gym environment for FrozenLake, `state` refers to the current state of the game, and `action` refers to the action that the agent is taking.\n",
    "\n",
    "The line `print(env.P[state][action])` is printing the transition probabilities for the current state and the selected action. The `env.P` attribute is a dictionary that maps state-action pairs to a list of transition tuples. Each transition tuple has the following format:\n",
    "\n",
    "`(probability, next_state, reward, done or termonal state)`\n",
    "\n",
    "where:\n",
    "\n",
    "* `probability` is the probability of transitioning to `next_state` when taking `action` in `state`.\n",
    "* `next_state` is the state that the environment will transition to.\n",
    "* `reward` is the reward that the agent will receive for transitioning to `next_state`.\n",
    "* `done` is a boolean that indicates whether the episode is over.\n",
    "\n",
    "So in the example you provided, `state=3` and `action=1`, and the output of `print(env.P[state][action])` is:\n",
    "\n",
    "`[(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]`\n",
    "\n",
    "This means that there are three possible outcomes when the agent takes action 1 in state 3:\n",
    "\n",
    "* With a probability of 1/3, the agent will transition to state 2 and receive a reward of 0. The episode will not be over.\n",
    "* With a probability of 1/3, the agent will transition to state 7 (which is a hole) and receive a reward of 0. The episode will be over.\n",
    "* With a probability of 1/3, the agent will remain in state 3 and receive a reward of 0. The episode will not be over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 3, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "state=3\n",
    "action=1\n",
    "print(env.P[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate an episode in the FrozenLake environment, you can follow these steps:\n",
    "\n",
    "1. Reset the environment to the initial state using the `env.reset()` method.\n",
    "2. Create a variable to keep track of whether the episode is over or not. For example, you can set `done = False`.\n",
    "3. While the episode is not over, repeat the following steps:\n",
    "\n",
    "a. Choose an action to take based on the current state of the environment. For example, you can use `env.action_space.sample()` to choose a random action.\n",
    "\n",
    "b. Take the chosen action in the environment using the `env.step()` method. This method returns a tuple that contains the next state of the environment, the reward for transitioning to the next state, a boolean that indicates whether the episode is over, and some additional information.\n",
    "\n",
    "c. Update the `done` variable based on the value of the `done` element in the tuple returned by `env.step()`.\n",
    "\n",
    "d. Optionally, you can render the environment to the screen using the `env.render()` method.\n",
    "\n",
    "Here's some sample code that generates an episode in the FrozenLake environment:\n",
    "```python\n",
    "import gym\n",
    "\n",
    "# Create the FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Reset the environment to the initial state\n",
    "state = env.reset()\n",
    "\n",
    "# Create a variable to keep track of whether the episode is over\n",
    "done = False\n",
    "\n",
    "# While the episode is not over, repeat the following steps\n",
    "while not done:\n",
    "    # Choose an action to take based on the current state\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Take the chosen action in the environment\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Update the state and done variables\n",
    "    state = next_state\n",
    "\n",
    "    # Optionally, render the environment to the screen\n",
    "    env.render()\n",
    "\n",
    "# Print the reward for the episode\n",
    "print(f\"Episode ended with reward: {reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "```\n",
    "This code generates a single episode in the FrozenLake environment using a random policy. The reward for the episode is printed to the console. The environment is rendered to the screen, but you can comment out the `env.render()` line to disable rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0 False False {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create the FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\") # Text-based rendering\n",
    "# or\n",
    "# env = gym.make(\"FrozenLake-v1\") # Image rendering (requires additional processing)\n",
    "# Reset the environment to the initial state\n",
    "env.reset()\n",
    "random_action= env.action_space.sample()\n",
    "next_state, reward, done, truncated, info = env.step(random_action)\n",
    "print( next_state, reward, done, truncated, info )\n",
    "# Render the environment to the screen\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main loops for creating epsides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Action: 1\n",
      "Episode 0, Action: 2\n",
      "Episode 0, Action: 0\n",
      "Episode 0, Action: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes:  20%|██        | 1/5 [00:01<00:05,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Action: 2\n",
      "Episode 1, Action: 0\n",
      "Episode 1, Action: 3\n",
      "Episode 1, Action: 3\n",
      "Episode 1, Action: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes:  40%|████      | 2/5 [00:02<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Action: 0\n",
      "Episode 2, Action: 0\n",
      "Episode 2, Action: 1\n",
      "Episode 2, Action: 2\n",
      "Episode 2, Action: 1\n",
      "Episode 2, Action: 0\n",
      "Episode 2, Action: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes:  60%|██████    | 3/5 [00:04<00:03,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3, Action: 3\n",
      "Episode 3, Action: 2\n",
      "Episode 3, Action: 1\n",
      "Episode 3, Action: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes:  80%|████████  | 4/5 [00:06<00:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, Action: 2\n",
      "Episode 4, Action: 1\n",
      "Episode 4, Action: 3\n",
      "Episode 4, Action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes: 100%|██████████| 5/5 [00:07<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Create the FrozenLake environment with RGB_ARRAY rendering mode\n",
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to get the initial state\n",
    "env.reset()\n",
    "env.render()\n",
    "episodes = 5\n",
    "\n",
    "# Wrap your range(episodes) with tqdm for a progress bar\n",
    "for i in tqdm(range(episodes), desc='Running Episodes'):\n",
    "    env.reset()\n",
    "    while True:\n",
    "        # Sample an action randomly\n",
    "        action = env.action_space.sample()\n",
    "        # Print the action\n",
    "        print(f'Episode {i}, Action: {action}')\n",
    "        # Execute the action and observe the new state and reward\n",
    "        next_state, reward, done, info,_ = env.step(action)\n",
    "        # print(next_state, reward, done, info)\n",
    "        if done: break \n",
    "        \n",
    "\n",
    "# Close the environment after running all the episodes\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.kernel.restart();",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "\n",
    "def restart_kernel():\n",
    "  display(Javascript('IPython.notebook.kernel.restart();'))\n",
    "\n",
    "restart_kernel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cart pendulam example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! **CartPole-v1** is a classic control problem that's part of the reinforcement learning environment in Gym, a toolkit for developing and comparing reinforcement learning algorithms. Here's a detailed explanation:\n",
    "\n",
    "### Environment Description\n",
    "In the **CartPole-v1** environment, a pole is attached by an unactuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force to the cart: either to the left or to the right¹. The pendulum starts upright, and the goal is to prevent it from falling over by moving the cart underneath it to keep it balanced⁴.\n",
    "\n",
    "### Action Space\n",
    "The action space is discrete with two possible actions¹:\n",
    "- **0**: Push the cart to the left\n",
    "- **1**: Push the cart to the right\n",
    "\n",
    "### Observation Space\n",
    "The observation is an array with four values¹:\n",
    "1. Cart Position: Ranges from -4.8 to 4.8\n",
    "2. Cart Velocity: Can be any real value\n",
    "3. Pole Angle: Approximately -0.418 radians (-24°) to 0.418 radians (24°)\n",
    "4. Pole Angular Velocity: Can be any real value\n",
    "\n",
    "### Rewards\n",
    "A reward of **+1** is provided for every timestep that the pole remains upright. The task is episodic, and the goal is to maximize the total reward, which is the length of time the pole stays up¹.\n",
    "\n",
    "### Episode Termination\n",
    "The episode ends if:\n",
    "- The pole angle is more than ±12°.\n",
    "- The cart position is more than ±2.4 (the center of the cart reaches the edge of the display).\n",
    "- The episode length is greater than 500 steps¹.\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniformly random value in the range (-0.05, 0.05) at the start of each episode¹.\n",
    "\n",
    "### Version Differences\n",
    "The **v1** version of CartPole has a reward threshold of 475 for solving, whereas the **v0** version has a threshold of 195¹.\n",
    "\n",
    "This environment is widely used as a benchmark and starting point for new reinforcement learning algorithms and is a great way to get familiar with the concepts of state, action, and reward in the context of an episodic task. It's also a common example used in various tutorials and introductions to reinforcement learning, such as those using Deep Q-Networks (DQN)².\n",
    "\n",
    "Source: Conversation with Copilot, 5/26/2024\n",
    "(1) Cart Pole - Gym Documentation. https://www.gymlibrary.dev/environments/classic_control/cart_pole/.\n",
    "(2) GitHub - alexandrulita91/cartpole-v1: A pole is attached by an un .... https://github.com/alexandrulita91/cartpole-v1.\n",
    "(3) Reinforcement Learning (DQN) Tutorial - PyTorch. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html.\n",
    "(4) OpenAI-Gym_CartPole-v1 - GitHub Pages. https://robert-haas.github.io/g3p/media/notebooks/cartpole.html.\n",
    "(5) Using Q-Learning for OpenAI’s CartPole-v1 - Medium. https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df.\n",
    "(6) Solving CartPole-V1. Cartpole Problem | by Siddharth Kale - Medium. https://medium.com/@siddharthkale/solving-cartpole-v1-4be909b7c2c6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Episodes: 100%|██████████| 200/200 [01:37<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "episodes = 200\n",
    "\n",
    "# Function to run one episode\n",
    "def run_episode(env):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Sample an action\n",
    "        next_state, reward, done, info,_ = env.step(action)    # Take the action\n",
    "\n",
    "# Main loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(env.observation_space)\n",
    "    print(env.action_space)\n",
    "    \n",
    "    time.sleep(2)  # Initial delay before starting the episodes\n",
    "    \n",
    "    # Run the episodes with a progress bar\n",
    "    for _ in tqdm(range(episodes), desc='Running Episodes'):\n",
    "        run_episode(env)\n",
    "    \n",
    "    time.sleep(5)  # Delay before closing the environment\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
