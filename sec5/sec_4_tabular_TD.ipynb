{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Difference (TD) learning is a reinforcement learning method that combines ideas from Monte Carlo and Dynamic Programming methods. It allows learning to occur from raw experience without a model of the environment's dynamics. TD learning updates estimates based on other learned estimates, without waiting for a final outcome, which is known as bootstrappingÂ¹.\n",
    "\n",
    "In tabular TD learning, specifically the TD(0) algorithm, the value function for a given policy is updated after every step from experience. Unlike Monte Carlo methods, which wait until the end of an episode to update the value function, TD methods update the value function at every time step.\n",
    "\n",
    "Here's a simple pseudocode for the tabular TD(0) algorithm, formatted as if it were in a Jupyter Notebook markdown cell:\n",
    "\n",
    "```latex\n",
    "# Tabular Temporal Difference (TD) Learning - TD(0) Pseudocode\n",
    "\n",
    "1. Initialize \\( V(s) \\) arbitrarily for all \\( s \\in S \\)\n",
    "2. Repeat (for each episode):\n",
    "    2.1 Initialize \\( s \\)\n",
    "    2.2 Repeat (for each step of episode):\n",
    "        2.2.1 Choose \\( a \\) from \\( s \\) using policy derived from \\( V \\) (e.g., epsilon-greedy)\n",
    "        2.2.2 Take action \\( a \\), observe reward \\( r \\), and next state \\( s' \\)\n",
    "        2.2.3 \\( V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)] \\)\n",
    "        2.2.4 \\( s \\leftarrow s' \\)\n",
    "    2.3 until \\( s \\) is terminal\n",
    "```\n",
    "\n",
    "In the pseudocode:\n",
    "- \\( V(s) \\) represents the value function for state \\( s \\).\n",
    "- \\( \\alpha \\) is the learning rate.\n",
    "- \\( \\gamma \\) is the discount factor.\n",
    "- \\( r \\) is the reward received after taking action \\( a \\).\n",
    "- \\( s' \\) is the new state after taking action \\( a \\).\n",
    "\n",
    "The update rule \\( V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)] \\) is the core of the TD(0) algorithm, where the value function \\( V(s) \\) is updated towards the estimated return \\( r + \\gamma V(s') \\).\n",
    "\n",
    "Source: Conversation with Copilot, 6/11/2024\n",
    "(1) Chapter 6: Temporal Difference Learning - Stanford University. https://web.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/11-12-TD.pdf.\n",
    "(2) Temporal difference learning - Wikipedia. https://en.wikipedia.org/wiki/Temporal_difference_learning.\n",
    "(3) Analysis of Temporal Difference Learning: Linear System Approach. https://arxiv.org/abs/2204.10479v5.\n",
    "(4) Temporal difference learning (TD Learning) | Engati. https://www.engati.com/glossary/temporal-difference-learning.\n",
    "(5) Temporal Difference Learning | SpringerLink. https://link.springer.com/chapter/10.1007/978-1-4842-9606-6_5.\n",
    "(6) Temporal-Difference Learning | Littleroot - Trung's Place. https://trunghng.github.io/posts/reinforcement-learning/td-learning/.\n",
    "(7) Temporal Difference Methods for Control - GitHub Pages. https://armahmood.github.io/rlcourse/lectures_w2020/week_9_mar_2.pdf.\n",
    "(8) A Meta-learning Method Based on Temporal Difference Error. https://link.springer.com/chapter/10.1007/978-3-642-10677-4_60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import  defaultdict\n",
    "env= gym.make(\"FrozenLake-v1\")\n",
    "def policy(state):\n",
    "    if state in [1,4]:\n",
    "        return 0\n",
    "    elif state in [2,9,14]:\n",
    "        return 1\n",
    "    elif state in [0,6,13]:\n",
    "        return 2\n",
    "    else: return 3\n",
    "    \n",
    "def st_policy(state):\n",
    "    if int(np.random.choice([0, 1], p=[0.2, 0.8])) == 1: return policy(state)\n",
    "    else:return int(np.random.choice([0, 1, 2,3])) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roozu\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "alfa= 0.05\n",
    "def TD_learing(policy= policy,eps =10000,alfa= 0.2,gamma=1):\n",
    "        V= defaultdict(int)\n",
    "        for _ in range(eps):\n",
    "            st,prob= env.reset()\n",
    "            while True:\n",
    "                At= st_policy(st)\n",
    "                st1, rt, done, _, _ = env.step(At)\n",
    "                V[st]= V[st]+alfa*(rt+gamma*V[st1]-V[st])\n",
    "                st=st1\n",
    "                if done: break\n",
    "        return V\n",
    "            \n",
    "            \n",
    "    \n",
    "V=TD_learing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.061435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.096136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.159050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.336053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.063388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.074195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.057226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.548987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.811846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state     value\n",
       "0       0  0.061435\n",
       "1       4  0.096136\n",
       "2       5  0.000000\n",
       "3       8  0.159050\n",
       "4       9  0.336053\n",
       "5      10  0.063388\n",
       "6       6  0.074195\n",
       "7       2  0.057226\n",
       "8       3  0.051443\n",
       "9       1  0.026116\n",
       "10     12  0.000000\n",
       "11      7  0.000000\n",
       "12     11  0.000000\n",
       "13     13  0.548987\n",
       "14     14  0.811846\n",
       "15     15  0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_df= pd.DataFrame(V.items(),columns=[\"state\",\"value\"])\n",
    "V_df.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(10):\n",
    "    print(int(np.random.choice([0, 1], p=[0.2, 0.8])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
