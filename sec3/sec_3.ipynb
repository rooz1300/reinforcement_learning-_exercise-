{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "1. **Initialization**\n",
    "    - Initialize policy $\\pi$ arbitrarily (e.g., $\\pi(s) = \\text{argmax}_a \\{ R_a(s) \\}$ for all $s \\in S$)\n",
    "    - Initialize utility $U(s)$ arbitrarily for all $s \\in S$\n",
    "\n",
    "2. **Policy Evaluation**\n",
    "    - Repeat until convergence:\n",
    "        - For each state $s \\in S$:\n",
    "            - $U(s) \\leftarrow R_{\\pi(s)}(s) + \\gamma \\sum_{s'} P_{\\pi(s)}(s, s') U(s')$\n",
    "\n",
    "3. **Policy Improvement**\n",
    "    - For each state $s \\in S$:\n",
    "        - $\\pi'(s) \\leftarrow \\text{argmax}_a \\{ R_a(s) + \\gamma \\sum_{s'} P_a(s, s') U(s') \\}$\n",
    "    - If $\\pi' = \\pi$, stop and return $\\pi$ and $U$\n",
    "    - Otherwise, set $\\pi \\leftarrow \\pi'$ and go to step 2\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "\n",
    "1. **Initialization**\n",
    "    - Initialize utility $U(s)$ arbitrarily for all $s \\in S$\n",
    "\n",
    "2. **Bellman Optimality Update**\n",
    "    - Repeat until convergence:\n",
    "        - For each state $s \\in S$:\n",
    "            - $U(s) \\leftarrow \\max_a \\{ R_a(s) + \\gamma \\sum_{s'} P_a(s, s') U(s') \\}$\n",
    "\n",
    "3. **Derive Policy from Utility**\n",
    "    - For each state $s \\in S$:\n",
    "        - $\\pi(s) \\leftarrow \\text{argmax}_a \\{ R_a(s) + \\gamma \\sum_{s'} P_a(s, s') U(s') \\}$\n",
    "    - Return $\\pi$ and $U$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**policy evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenAI Gym, the number of actions available in an environment depends on the type of action space it has. Here's how you can check the number of actions:\n",
    "\n",
    "- For a **discrete** and one-dimensional action space, you can use the following code to get the number of actions:\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "a = env.action_space\n",
    "print(a)                    # prints Discrete(3)\n",
    "print(a.n)                  # prints 3\n",
    "```\n",
    "This will print the number of actions available, which is simply an integer¹.\n",
    "\n",
    "- If the action space is **multi-dimensional** and discrete, you would use `nvec` to get an array describing the number of available actions for each dimension:\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(\"YourEnvHere\")\n",
    "print(env.action_space.nvec) # prints array([5, 5, 5, 5], dtype=int64) for a MultiDiscrete space\n",
    "```\n",
    "This gives you a numpy array with the number of actions available in each dimension².\n",
    "\n",
    "- For a **continuous** action space, you would get a `Box` object, and you can access its properties like so:\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "a = env.action_space\n",
    "print(a)                    # prints Box(1,)\n",
    "print(a.shape)              # prints (1,)\n",
    "print(a.high)               # prints [1.] for the maximum value\n",
    "print(a.low)                # prints [-1.] for the minimum value\n",
    "```\n",
    "This will give you the shape of the action space and the bounds for each dimension¹.\n",
    "\n",
    "As for the dates, if you're referring to the release dates or update dates for OpenAI Gym environments, this information isn't provided in the search results. You might want to check the official OpenAI Gym GitHub repository or the release notes for the most accurate and up-to-date information.\n",
    "\n",
    "Source: Conversation with Copilot, 5/30/2024\n",
    "(1) How to check the actions available in OpenAI gym environment?. https://stackoverflow.com/questions/62303994/how-to-check-the-actions-available-in-openai-gym-environment.\n",
    "(2) OpenAI Gym: Walk through all possible actions in an action space. https://stackoverflow.com/questions/64588828/openai-gym-walk-through-all-possible-actions-in-an-action-space.\n",
    "(3) How to get the number of valid actions for a descrete action space of .... https://github.com/openai/gym/issues/2005.\n",
    "(4) undefined. https://github.com/openai/multiagent-particle-envs/blob/master/multiagent/environment.py.\n",
    "To find out the number of states in an OpenAI Gym environment, you can check the `observation_space` attribute. This attribute describes the format of valid observations for the environment. Here's how you can do it:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(\"YourEnvNameHere\")\n",
    "print(env.observation_space)\n",
    "```\n",
    "\n",
    "For discrete state spaces, this will typically return a `Discrete` object with a single integer indicating the number of possible states. For example, in the Taxi-v2 environment, there are **500 states**³.\n",
    "\n",
    "For continuous state spaces, you'll get a `Box` object, which represents an n-dimensional box, so you can check the `shape` attribute to find out the size of the state space:\n",
    "\n",
    "```python\n",
    "print(env.observation_space.n)\n",
    "```\n",
    "\n",
    "This will give you a tuple representing the dimensions of the state space. Keep in mind that for continuous spaces, the number of states is technically infinite, as the states are represented by real numbers within given bounds.\n",
    "\n",
    "If you're looking for the specific number of states for a particular environment, you might need to refer to the environment's documentation or source code, as this information is not always explicitly stated. For instance, the Taxi-v2 environment has a total of **500 states** as mentioned earlier³.\n",
    "\n",
    "Source: Conversation with Copilot, 5/30/2024\n",
    "(1) What are the different states in Open AI Taxi Environment?. https://stats.stackexchange.com/questions/359638/what-are-the-different-states-in-open-ai-taxi-environment.\n",
    "(2) How to set a openai-gym environment start with a specific state not the .... https://stackoverflow.com/questions/57839665/how-to-set-a-openai-gym-environment-start-with-a-specific-state-not-the-env-res.\n",
    "(3) Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/.\n",
    "(4) Getting Started With OpenAI Gym | Built In. https://builtin.com/software-engineering-perspectives/openai-gym.\n",
    "(5) How to check the actions available in OpenAI gym environment?. https://stackoverflow.com/questions/62303994/how-to-check-the-actions-available-in-openai-gym-environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n",
      "93\n",
      "[[0.01235614 0.01042446 0.01933844 0.00947775]\n",
      " [0.01478705 0.         0.03889445 0.        ]\n",
      " [0.03260247 0.08433764 0.13781085 0.        ]\n",
      " [0.         0.17034482 0.43357944 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import gym \n",
    "\n",
    "# Create the FrozenLake environment\n",
    "env= gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Get the number of states and actions from the environment\n",
    "number_states= env.observation_space.n\n",
    "number_action= env.action_space.n\n",
    "\n",
    "# Initialize a random policy\n",
    "random_policy= np.ones((number_states,number_action))/number_action \n",
    "\n",
    "# Set the discount factor, the small threshold theta, and the maximum number of iterations\n",
    "discount_factor =0.99\n",
    "theta=1e-10\n",
    "max_iteration=20000\n",
    "\n",
    "# Print the shape of the random policy\n",
    "print(random_policy.shape)\n",
    "\n",
    "# Define the policy evaluation function\n",
    "def policy_evaluaton(policy=random_policy,gamma=discount_factor,theta=theta,max_iteration=max_iteration ):\n",
    "    # Initialize the value function\n",
    "    pre_v =np.zeros(number_states)\n",
    "    iteration=0\n",
    "    delta=0\n",
    "    while True:\n",
    "        iteration+=1 \n",
    "        # Initialize the new value function\n",
    "        V=np.zeros(number_states)\n",
    "        # For each state\n",
    "        for state in range(number_states):\n",
    "            # For each action\n",
    "            for action in range(number_action):\n",
    "                # For each possible next state\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # Update the value function\n",
    "                    V[state]+= policy[state][action]*prob*(reward+gamma*pre_v[next_state])\n",
    "\n",
    "        # Calculate the maximum change in value function\n",
    "        delta=np.max(np.abs(V-pre_v))\n",
    "        # Update the value function\n",
    "        pre_v= np.copy(V)\n",
    "\n",
    "        # If the maximum change in value function is less than the threshold, stop. \n",
    "        # Also stop if the number of iterations has reached the maximum limit\n",
    "        if delta < theta or iteration >= max_iteration :break\n",
    "    return V,iteration\n",
    "\n",
    "# Evaluate the policy and print the value function\n",
    "V,i=policy_evaluaton() \n",
    "print(i)\n",
    "print(V.reshape((4,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating  Value functions for a deterministic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09554433 0.04705915 0.0470064  0.04562386]\n",
      " [0.1469248  0.         0.04976062 0.        ]\n",
      " [0.20275753 0.26473443 0.10378337 0.        ]\n",
      " [0.         0.49568466 0.74165563 0.        ]]\n",
      "iterastin:218\n"
     ]
    }
   ],
   "source": [
    "deterministic_policy = np.array([[0, 0, 1, 0],\n",
    "[1, 0, 0, 0],\n",
    "[0, 1, 0, 0],\n",
    "[0, 0, 0, 1],\n",
    "[1, 0, 0, 0],\n",
    "[1, 0, 0, 0],\n",
    "[0, 0, 1, 0],\n",
    "[1, 0, 0, 0],\n",
    "[0, 0, 0, 1],\n",
    "[0, 1, 0, 0],\n",
    "[0, 0, 0, 1],\n",
    "[1, 0, 0, 0],\n",
    "[1, 0, 0, 0],\n",
    "[0, 0, 1, 0],\n",
    "[0, 1, 0, 0],\n",
    "[0, 1, 0, 0]])\n",
    "V,itera =policy_evaluaton(policy=deterministic_policy)\n",
    "print(V.reshape((4,4)))\n",
    "print(f'iterastin:{itera}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(V=V, gamma=discount_factor):\n",
    "    # Initialize the policy matrix with zeros\n",
    "    policy = np.zeros((number_states, number_action))\n",
    "    \n",
    "    # Initialize the Q-value matrix with zeros\n",
    "    Q = np.zeros((number_states, number_action))\n",
    "    \n",
    "    # Iterate over all states\n",
    "    for state in range(number_states):\n",
    "        # Iterate over all possible actions\n",
    "        for action in range(number_action):\n",
    "            # Calculate the Q-value for each state-action pair\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                # Update the Q-value based on the transition probabilities, rewards, and discount factor\n",
    "                Q[state][action] += prob * (reward + gamma * V[next_state])\n",
    "    \n",
    "    # Find the index (action) of the max Q-value for each state\n",
    "    max_index = np.argmax(Q, axis=1)\n",
    "    \n",
    "    # Construct the greedy policy based on the max Q-values\n",
    "    for i, optimal_action in enumerate(max_index):\n",
    "        # Set the action with the highest Q-value to 1 (indicating it's the chosen action)\n",
    "        policy[i][optimal_action] = 1\n",
    "    \n",
    "    # Return the greedy policy and the Q-value matrix\n",
    "    return policy, Q\n",
    "\n",
    "\n",
    "policy, Q=greedy_policy(V,gamma=discount_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a random policy\n",
    "def policy_iteration(policy=random_policy):\n",
    "    old_policy=policy\n",
    "    iteration=0\n",
    "    while True:\n",
    "        V,_= policy_evaluaton(policy=old_policy)\n",
    "        new_policy, Q=greedy_policy(V=V, gamma=discount_factor)\n",
    "        \n",
    "      \n",
    "        comparison= (old_policy==new_policy)\n",
    "        if comparison.all()==True: break\n",
    "        \n",
    "        old_policy=new_policy\n",
    "        iteration+=1\n",
    "    return new_policy, iteration\n",
    "\n",
    "random_policy= np.random.rand(number_states,number_action) \n",
    "optimal_policy_politer, iteration=policy_iteration()        \n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy_politer)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 0\n",
      "Chosen action: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roozu\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 0, Reward: 0.0, Done: False\n",
      "Current state: 0\n",
      "Chosen action: 0\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 9, Reward: 0.0, Done: False\n",
      "Current state: 9\n",
      "Chosen action: 1\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 4, Reward: 0.0, Done: False\n",
      "Current state: 4\n",
      "Chosen action: 0\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 8, Reward: 0.0, Done: False\n",
      "Current state: 8\n",
      "Chosen action: 3\n",
      "New state: 9, Reward: 0.0, Done: False\n",
      "Current state: 9\n",
      "Chosen action: 1\n",
      "New state: 13, Reward: 0.0, Done: False\n",
      "Current state: 13\n",
      "Chosen action: 2\n",
      "New state: 14, Reward: 0.0, Done: False\n",
      "Current state: 14\n",
      "Chosen action: 1\n",
      "New state: 15, Reward: 1.0, Done: True\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "def run_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # For newer versions of gym that return a tuple (state, info)\n",
    "    env.render()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Print the current state for debugging\n",
    "        print(f\"Current state: {state}\")\n",
    "        \n",
    "        # Ensure the state is an integer index for the policy array\n",
    "        try:\n",
    "            action = np.argmax(policy[state])\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError: {e}\")\n",
    "            print(f\"Policy shape: {policy.shape}, state: {state}\")\n",
    "            break\n",
    "        \n",
    "        # Print the chosen action for debugging\n",
    "        print(f\"Chosen action: {action}\")\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        step_result = env.step(action)\n",
    "        if len(step_result) == 4:\n",
    "            next_state, reward, done, info = step_result\n",
    "        else:\n",
    "            next_state, reward, done, truncated, info = step_result\n",
    "            done = done or truncated\n",
    "        \n",
    "        if isinstance(next_state, tuple):\n",
    "            next_state = next_state[0]  # For newer versions of gym that return a tuple (state, info)\n",
    "        \n",
    "        # Render the new state of the environment\n",
    "        env.render()\n",
    "        print(f\"New state: {next_state}, Reward: {reward}, Done: {done}\")\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "\n",
    "\n",
    "# Run the policy on the environment\n",
    "run_policy(env, optimal_policy_politer)\n",
    "time.sleep(10)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**value policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roozu\\AppData\\Local\\Temp\\ipykernel_13944\\320221126.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[state][action] += prob * (reward + gamma * pre_V[next_state])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import gym \n",
    "\n",
    "# Create the FrozenLake environment\n",
    "env= gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Get the number of states and actions from the environment\n",
    "number_states= env.observation_space.n\n",
    "number_action= env.action_space.n\n",
    "gamma =0.99\n",
    "theta= 1e-6 #stoping condition \n",
    "\n",
    "def value_iterstion():\n",
    "    policy= np.zeros((number_states,number_action))\n",
    "    pre_V= np.zeros((number_states,1))\n",
    "    \n",
    "    iteration=0\n",
    "    while True:\n",
    "        Q= np.zeros((number_states,number_action))\n",
    "        \n",
    "        for state in range(number_states):\n",
    "            for action in range(number_action):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # Update the Q-value based on the transition probabilities, rewards, and discount factor\n",
    "                    Q[state][action] += prob * (reward + gamma * pre_V[next_state])\n",
    "        V=np.max(Q,axis=1)\n",
    "        if np.max(np.max(np.abs(pre_V-V))) < theta: \n",
    "            break\n",
    "        pre_V = V.copy()\n",
    "        iteration += 1\n",
    "    return V, Q,iteration\n",
    "\n",
    "V, Q,iteration=value_iterstion()\n",
    "optimal_policy=np.argmax(Q,axis=1)\n",
    "print(np.argmax(Q,axis=1))\n",
    "print(np.argmax(optimal_policy_politer,axis=1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make this code more clearer and readable by adding commment\n",
    "\n",
    "# import numpy as np\n",
    "# import gym \n",
    "# env= gym.make(\"FrozenLake-v1\")\n",
    "# number_states= env.observation_space.n\n",
    "# number_action= env.action_space.n\n",
    "# random_policy= np.ones((number_states,number_action))/number_action \n",
    "# discount_factor =1\n",
    "# theta=0.0001\n",
    "# max_iteration=20000\n",
    "# print(random_policy.shape)\n",
    "# def policy_evaluaton(policy=random_policy,gamma=discount_factor,theta=theta,max_iteration=max_iteration ):\n",
    "#     pre_v =np.zeros(number_states)\n",
    "#     i=1  \n",
    "#     while True:\n",
    "#         # print(\"iteation\",i)\n",
    "#         i+=1\n",
    "#         delta=0\n",
    "#         V=np.zeros(number_states)\n",
    "#         for state in range(number_states):\n",
    "#             # print(\"current state: \",state)\n",
    "#             for action in range(number_action):\n",
    "#                 for prob, next_state, reward, done in env.P[state][action]:\n",
    "#                     # print(prob, next_state)\n",
    "#                     V[state]+= policy[state][action]*prob*(reward+gamma*pre_v[next_state])\n",
    "\n",
    "#         delta=np.max(np.abs(V-pre_v))\n",
    "#         pre_v= np.copy(V)\n",
    "\n",
    "#         if delta < theta or i >= max_iteration :break\n",
    "#     return V,i\n",
    "\n",
    "# V,i=policy_evaluaton() \n",
    "# print(V)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def policy_evaluation(policy, gamma, number_states, number_actions, env, theta=0.01, max_iterations=200):\n",
    "#     \"\"\"\n",
    "#     Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "#     Args:\n",
    "#         policy: [S, A] shaped matrix representing the policy.\n",
    "#         gamma: Discount factor.\n",
    "#         number_states: Number of states in the environment.\n",
    "#         number_actions: Number of actions in the environment.\n",
    "#         env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "#         theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "#         max_iterations: Maximum number of iterations to prevent infinite loops.\n",
    "    \n",
    "#     Returns:\n",
    "#         Vector of length number_states representing the value function.\n",
    "#     \"\"\"\n",
    "#     V = np.zeros(number_states)\n",
    "#     for i in tqdm(range(max_iterations), desc='Policy Evaluation Progress'):\n",
    "#         prev_V = np.copy(V)\n",
    "#         for state in range(number_states):\n",
    "#             V[state] = sum([policy[state][action] * sum([prob * (reward + gamma * prev_V[next_state])\n",
    "#                              for prob, next_state, reward, _ in env.P[state][action]])\n",
    "#                              for action in range(number_actions)])\n",
    "#         delta = np.max(np.abs(prev_V - V))\n",
    "#         if delta < theta:\n",
    "#             print(f'Policy evaluated in {i+1} iterations.')\n",
    "#             break\n",
    "#     return V\n",
    "        \n",
    "# policy_evaluaton()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policy_evaluaton(policy=random_policy,gamma=discount_factor ):\n",
    "#     pre_v =np.zeros(number_states)\n",
    "#     delta=0\n",
    "#     theta= 0.01\n",
    "#     i=1  \n",
    "    \n",
    "#     while True:\n",
    "#         print(\"iteation\",i)\n",
    "#         i+=1\n",
    "#         for state in range(number_states):\n",
    "#             # print(\"current state: \",state)\n",
    "#             delta=0\n",
    "#             V=np.zeros(number_states)\n",
    "#             for action in range(number_action):\n",
    "#                 for prob, next_state, reward, done in env.P[state][action]:\n",
    "#                     # print(prob, next_state)\n",
    "#                     V[state]+= policy[state][action]*prob*(reward+gamma*pre_v[next_state])\n",
    "\n",
    "#         delta=np.max(np.abs(V-pre_v))\n",
    "#         pre_v= np.copy(V)\n",
    "#         print(delta)\n",
    "#         print(V)\n",
    "#         print(pre_v)\n",
    "#         if delta < theta :break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#         # error= 0.001    \n",
    "#         # if delta <= error: break\n",
    "        \n",
    "        \n",
    "# policy_evaluaton()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# Set the discount factor, the small threshold theta, and the maximum number of iterations\n",
    "# discount_factor =0.99\n",
    "# theta=1e-2\n",
    "# max_iteration=2000\n",
    "# policy_pre=random_policy\n",
    "# iteration=1\n",
    "# while True:\n",
    "#     iteration+=1\n",
    "#     V_pre,_= policy_evaluaton(policy=policy_pre,gamma=discount_factor,theta=theta,max_iteration=max_iteration )\n",
    "#     new_policy,_= greedy_policy(V=V_pre, gamma=discount_factor)\n",
    "#     V_new,_= policy_evaluaton(policy=new_policy,gamma=discount_factor,theta=theta,max_iteration=max_iteration )\n",
    "#     delta=np.max(np.abs(V_new-V_pre))\n",
    "#     if delta < theta or iteration >= max_iteration :break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
